{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\jassu\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (2.4.0)\n",
      "Requirement already satisfied: responses<0.19 in c:\\users\\jassu\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from datasets) (0.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\jassu\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from datasets) (1.19.5)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\jassu\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from datasets) (4.64.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\jassu\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\jassu\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from datasets) (0.70.12.2)\n",
      "Requirement already satisfied: dill<0.3.6 in c:\\users\\jassu\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from datasets) (0.3.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\jassu\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from datasets) (1.1.5)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in c:\\users\\jassu\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from datasets) (2022.1.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\jassu\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from datasets) (2.27.1)\n",
      "Requirement already satisfied: dataclasses in c:\\users\\jassu\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from datasets) (0.8)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\jassu\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from datasets) (4.8.3)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\jassu\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from datasets) (3.8.6)\n",
      "Requirement already satisfied: packaging in c:\\users\\jassu\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in c:\\users\\jassu\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in c:\\users\\jassu\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\jassu\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\jassu\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\jassu\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\jassu\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from packaging->datasets) (3.0.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\jassu\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from requests>=2.19.0->datasets) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jassu\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jassu\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\jassu\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.18)\n",
      "Requirement already satisfied: six in c:\\users\\jassu\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from responses<0.19->datasets) (1.15.0)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\jassu\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from tqdm>=4.62.1->datasets) (5.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\jassu\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\jassu\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from aiohttp->datasets) (1.7.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\jassu\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\jassu\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: idna-ssl>=1.0 in c:\\users\\jassu\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from aiohttp->datasets) (1.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\jassu\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from aiohttp->datasets) (5.2.0)\n",
      "Requirement already satisfied: asynctest==0.13.0 in c:\\users\\jassu\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from aiohttp->datasets) (0.13.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\jassu\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from aiohttp->datasets) (22.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\jassu\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\jassu\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from importlib-metadata->datasets) (3.6.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\jassu\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\jassu\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jassu\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jassu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Using custom data configuration default\n",
      "Reusing dataset xsum (C:\\Users\\jassu\\.cache\\huggingface\\datasets\\xsum\\default\\1.2.0\\32c23220eadddb1149b16ed2e9430a05293768cfffbdfd151058697d4c11f934)\n",
      "100%|██████████| 3/3 [00:00<00:00,  6.75it/s]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Download NLTK resources (stopwords)\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove special characters and digits\n",
    "    cleaned_text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Convert to lowercase\n",
    "    cleaned_text = cleaned_text.lower()\n",
    "    # Remove extra whitespaces\n",
    "    cleaned_text = ' '.join(cleaned_text.split())\n",
    "    return cleaned_text\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize the text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "    # Join the tokens back into a text string\n",
    "    preprocessed_text = ' '.join(filtered_tokens)\n",
    "    return preprocessed_text\n",
    "\n",
    "def process_dataset(dataset, tokenizer, max_len):\n",
    "    # Clean and preprocess document texts\n",
    "    documents = [preprocess_text(clean_text(doc)) for doc in dataset['document']]\n",
    "\n",
    "    # Clean and preprocess summaries\n",
    "    summaries = [preprocess_text(clean_text(summary)) for summary in dataset['summary']]\n",
    "\n",
    "    # Tokenize and pad sequences for documents\n",
    "    X_seq = tokenizer.texts_to_sequences(documents)\n",
    "    X_padded = pad_sequences(X_seq, maxlen=max_len, padding='post')\n",
    "\n",
    "    # Tokenize and pad sequences for summaries\n",
    "    Y_seq = tokenizer.texts_to_sequences(summaries)\n",
    "    Y_padded = pad_sequences(Y_seq, maxlen=max_len, padding='post')\n",
    "\n",
    "    return X_padded, Y_padded\n",
    "\n",
    "# Load the dataset\n",
    "multi_news_dataset = load_dataset('xsum')\n",
    "\n",
    "# Accessing the train, validation, and test splits\n",
    "train_dataset1 = multi_news_dataset['train']\n",
    "validation_dataset = multi_news_dataset['validation']\n",
    "\n",
    "test_dataset = multi_news_dataset['test']\n",
    "train_dataset, unused_data = train_test_split(train_dataset1, test_size=0.007, random_state=42)\n",
    "\n",
    "# Tokenize and preprocess the text data\n",
    "max_words = 10000\n",
    "max_len = 100  # Adjust as needed\n",
    "\n",
    "# Tokenizer for word indexing\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(train_dataset['document'])\n",
    "\n",
    "# Process training dataset\n",
    "X_train, Y_train = process_dataset(train_dataset, tokenizer, max_len)\n",
    "\n",
    "# Process validation dataset\n",
    "X_validation, Y_validation = process_dataset(validation_dataset, tokenizer, max_len)\n",
    "\n",
    "# Process test dataset\n",
    "X_test, Y_test = process_dataset(test_dataset, tokenizer, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add an end-of-sequence token (EOS) to the target sequences\n",
    "EOS_token = len(tokenizer.word_index) + 1  # Choose an index not used in the vocabulary\n",
    "Y_train_eos = np.column_stack([Y_train, np.full_like(Y_train[:, :1], EOS_token)])\n",
    "Y_validation_eos = np.column_stack([Y_validation, np.full_like(Y_validation[:, :1], EOS_token)])\n",
    "Y_test_eos = np.column_stack([Y_test, np.full_like(Y_test[:, :1], EOS_token)])\n",
    "\n",
    "# Actual labels for training\n",
    "actual_labels_train = Y_train_eos[:, 1:]\n",
    "\n",
    "# Actual labels for validation\n",
    "actual_labels_validation = Y_validation_eos[:, 1:]\n",
    "\n",
    "# Actual labels for testing\n",
    "actual_labels_test = Y_test_eos[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Seq2Seq model\n",
    "def seq2seq_model_with_eos(input_vocab_size, output_vocab_size, embedding_dim, hidden_units, max_len_input, max_len_output):\n",
    "    # Encoder\n",
    "    encoder_input = Input(shape=(max_len_input,))\n",
    "    encoder_embedding = Embedding(input_vocab_size, embedding_dim, input_length=max_len_input)(encoder_input)\n",
    "    _, state_h, state_c = LSTM(hidden_units, return_state=True)(encoder_embedding)\n",
    "    encoder_states = [state_h, state_c]\n",
    "\n",
    "    # Decoder\n",
    "    decoder_input = Input(shape=(max_len_output,))\n",
    "    decoder_embedding = Embedding(output_vocab_size, embedding_dim, input_length=max_len_output)(decoder_input)\n",
    "    decoder_lstm = LSTM(hidden_units, return_sequences=True, return_state=True)\n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "    decoder_dense = Dense(output_vocab_size, activation='softmax')\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "    model = Model([encoder_input, decoder_input], decoder_outputs)\n",
    "    return model\n",
    "\n",
    "# Vocabulary sizes\n",
    "input_vocab_size = len(tokenizer.word_index) + 1\n",
    "output_vocab_size = len(tokenizer.word_index) + 2  # Add one for EOS token\n",
    "\n",
    "# Model parameters\n",
    "embedding_dim = 50\n",
    "hidden_units = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 100, 50)      16091350    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 100, 50)      16091400    input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 100), (None, 60400       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 100, 100), ( 60400       embedding_1[0][0]                \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 100, 321828)  32504628    lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 64,808,178\n",
      "Trainable params: 64,808,178\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "model = seq2seq_model_with_eos(input_vocab_size, output_vocab_size, embedding_dim, hidden_units, max_len_input = 100, max_len_output = 100)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()\n",
    "\n",
    "# # Evaluate the model on the test set\n",
    "# evaluation = model.evaluate([X_test, actual_labels_test], Y_test_eos[:, 1:])\n",
    "# print(\"Test Loss:\", evaluation[0])\n",
    "# print(\"Test Accuracy:\", evaluation[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   8/4053 [..............................] - ETA: 495:08:08 - loss: 12.6700 - accuracy: 0.6698"
     ]
    }
   ],
   "source": [
    "model.fit([X_train, actual_labels_train], Y_train_eos[:, 1:], epochs=1, batch_size=50)  # Adjust the batch size as needed\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
