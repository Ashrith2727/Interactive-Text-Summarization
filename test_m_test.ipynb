{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     C:\\Users\\jassu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from nltk.corpus import reuters\n",
    "import nltk\n",
    "\n",
    "# Download the Reuters dataset\n",
    "nltk.download('reuters')\n",
    "\n",
    "# Load the Reuters dataset\n",
    "docs = reuters.fileids()\n",
    "train_docs_id = list(filter(lambda doc: doc.startswith(\"train\"), docs))\n",
    "test_docs_id = list(filter(lambda doc: doc.startswith(\"test\"), docs))\n",
    "\n",
    "train_docs = [reuters.raw(doc_id) for doc_id in train_docs_id]\n",
    "test_docs = [reuters.raw(doc_id) for doc_id in test_docs_id]\n",
    "\n",
    "# Tokenize and preprocess the text data\n",
    "max_words = 10000\n",
    "max_len = 100  # Adjust as needed\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(train_docs)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(train_docs)\n",
    "X_test_seq = tokenizer.texts_to_sequences(test_docs)\n",
    "\n",
    "X_train = pad_sequences(X_train_seq, maxlen=max_len, padding='post')\n",
    "X_test = pad_sequences(X_test_seq, maxlen=max_len, padding='post')\n",
    "\n",
    "# Create target summaries\n",
    "Y_train = tokenizer.texts_to_sequences([reuters.raw(doc_id).split('\\n')[0] for doc_id in train_docs_id])\n",
    "Y_test = tokenizer.texts_to_sequences([reuters.raw(doc_id).split('\\n')[0] for doc_id in test_docs_id])\n",
    "\n",
    "# Ensure that sequences have the same length as input sequences\n",
    "Y_train = pad_sequences(Y_train, maxlen=max_len, padding='post')\n",
    "Y_test = pad_sequences(Y_test, maxlen=max_len, padding='post')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "98/98 [==============================] - 1921s 20s/step - loss: 1.4776 - accuracy: 0.9160 - val_loss: 0.5656 - val_accuracy: 0.9245\n",
      "Epoch 2/10\n",
      "98/98 [==============================] - 1134s 11s/step - loss: 0.5251 - accuracy: 0.9278 - val_loss: 0.5427 - val_accuracy: 0.9290\n",
      "Epoch 3/10\n",
      "98/98 [==============================] - 966s 10s/step - loss: 0.5037 - accuracy: 0.9305 - val_loss: 0.5226 - val_accuracy: 0.9292\n",
      "Epoch 4/10\n",
      "98/98 [==============================] - 861s 9s/step - loss: 0.4695 - accuracy: 0.9322 - val_loss: 0.4781 - val_accuracy: 0.9336\n",
      "Epoch 5/10\n",
      "98/98 [==============================] - 994s 10s/step - loss: 0.4166 - accuracy: 0.9359 - val_loss: 0.4250 - val_accuracy: 0.9357\n",
      "Epoch 6/10\n",
      "98/98 [==============================] - 897s 9s/step - loss: 0.3620 - accuracy: 0.9399 - val_loss: 0.3769 - val_accuracy: 0.9406\n",
      "Epoch 7/10\n",
      "98/98 [==============================] - 914s 9s/step - loss: 0.3126 - accuracy: 0.9460 - val_loss: 0.3407 - val_accuracy: 0.9465\n",
      "Epoch 8/10\n",
      "98/98 [==============================] - 894s 9s/step - loss: 0.2764 - accuracy: 0.9510 - val_loss: 0.3132 - val_accuracy: 0.9509\n",
      "Epoch 9/10\n",
      "98/98 [==============================] - 1291s 13s/step - loss: 0.2485 - accuracy: 0.9558 - val_loss: 0.2855 - val_accuracy: 0.9566\n",
      "Epoch 10/10\n",
      "98/98 [==============================] - 996s 10s/step - loss: 0.2191 - accuracy: 0.9604 - val_loss: 0.2679 - val_accuracy: 0.9595\n",
      "95/95 [==============================] - 169s 2s/step - loss: 0.2501 - accuracy: 0.9613\n",
      "Test Loss: 0.25011926889419556, Test Accuracy: 0.9613050818443298\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Input, Bidirectional, Concatenate, MultiHeadAttention, TimeDistributed\n",
    "\n",
    "latent_size_1 = 256\n",
    "latent_size_2 = 256\n",
    "latent_size_3 = 128\n",
    "embedding_dim = 200\n",
    "batch_size = 64\n",
    "vocab_size = len(word_index) + 1\n",
    "num_heads = 4  # Number of attention heads\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(max_len,))\n",
    "embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_len)\n",
    "embedded_inputs = embedding_layer(encoder_inputs)\n",
    "\n",
    "encoder_lstm_1 = Bidirectional(LSTM(latent_size_1, return_sequences=True))(embedded_inputs)\n",
    "encoder_lstm_2 = Bidirectional(LSTM(latent_size_2, return_sequences=True))(encoder_lstm_1)\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = Input(shape=(max_len,))\n",
    "decoder_embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_len)\n",
    "decoder_embedded_inputs = decoder_embedding_layer(decoder_inputs)\n",
    "\n",
    "decoder_lstm_1 = LSTM(latent_size_2, return_sequences=True)\n",
    "decoder_lstm_2 = LSTM(latent_size_3, return_sequences=True)\n",
    "\n",
    "decoder_lstm_1_out = decoder_lstm_1(decoder_embedded_inputs)\n",
    "decoder_lstm_2_out = decoder_lstm_2(decoder_lstm_1_out)\n",
    "\n",
    "# Multi-head attention mechanism\n",
    "attention = MultiHeadAttention(num_heads=num_heads, key_dim=latent_size_3 // num_heads)(query=decoder_lstm_2_out, value=encoder_lstm_2)\n",
    "\n",
    "# Apply attention to each time step\n",
    "attention = TimeDistributed(Dense(latent_size_3))(attention)\n",
    "\n",
    "# Concatenate attention output and decoder LSTM output\n",
    "decoder_combined_context = Concatenate(axis=-1)([decoder_lstm_2_out, attention])\n",
    "\n",
    "decoder_dense = Dense(vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_combined_context)\n",
    "\n",
    "# Model\n",
    "seq2seq_Model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "seq2seq_Model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training\n",
    "seq2seq_Model.fit([X_train, Y_train], Y_train, epochs=10, batch_size=64, validation_split=0.2)\n",
    "\n",
    "# Evaluation\n",
    "loss, accuracy = seq2seq_Model.evaluate([X_test, Y_test], Y_test)\n",
    "print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
